# Issue #013: BackupService - –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
### –°—Ç–∞—Ç—É—Å: –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ

**üèóÔ∏è Milestone:** M3  
**üìà –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** LOW  
**üîó –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** Issue #6 (DatabaseService)

---

## üìù –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã

–ù–µ—Ç —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–∏ —Å–±–æ—è—Ö –∏–ª–∏ –ø–æ—Ç–µ—Ä–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–¥–µ–ª–æ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏.

### üîç –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –ë–î
- –ù–µ—Ç backup –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
- –ü–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ hardware —Å–±–æ—è—Ö
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ versioning –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
- –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è

### üéØ –ñ–µ–ª–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ backup –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
- Incremental –∏ full backup
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ backup —Ñ–∞–π–ª–æ–≤
- –ë—ã—Å—Ç—Ä–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- Versioning –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫

---

## üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```python
class BackupService:
    \"\"\"–°–ª—É–∂–±–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"
    
    def __init__(self, storage_backends: List[BackupStorage]):
        self.storage_backends = storage_backends
        self.scheduler = BackupScheduler()
        self.compression = BackupCompression()
        
    async def create_full_backup(self) -> BackupInfo:
    async def create_incremental_backup(self, since: datetime) -> BackupInfo:
    async def restore_from_backup(self, backup_id: str) -> bool:
    async def schedule_automatic_backups(self, schedule: BackupSchedule):
    async def list_available_backups(self) -> List[BackupInfo]:
    async def verify_backup_integrity(self, backup_id: str) -> bool:
    async def cleanup_old_backups(self, retention_policy: RetentionPolicy);

class BackupStorage:
    \"\"\"–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ backup\"\"\"
    
    async def store_backup(self, backup_data: bytes, metadata: BackupMetadata) -> str:
    async def retrieve_backup(self, backup_id: str) -> bytes:
    async def delete_backup(self, backup_id: str) -> bool:
    async def list_backups(self) -> List[BackupInfo]:

class BackupScheduler:
    \"\"\"–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ backup\"\"\"
    
    async def start_scheduler(self):
    async def stop_scheduler(self):
    async def add_schedule(self, schedule: BackupSchedule);
    async def remove_schedule(self, schedule_id: str);
```

### üìä –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö

```python
@dataclass
class BackupInfo:
    backup_id: str
    backup_type: str  # 'FULL', 'INCREMENTAL'
    created_at: datetime
    size_bytes: int
    checksum: str
    metadata: BackupMetadata
    storage_location: str

@dataclass
class BackupMetadata:
    database_version: str
    config_version: str
    included_tables: List[str]
    excluded_data: List[str]
    compression_type: str
    encryption_enabled: bool

@dataclass
class BackupSchedule:
    schedule_id: str
    backup_type: str
    cron_expression: str  # '0 2 * * *' for daily at 2 AM
    retention_days: int
    enabled: bool
    storage_backends: List[str]

@dataclass
class RetentionPolicy:
    daily_keep: int = 7     # Keep 7 daily backups
    weekly_keep: int = 4    # Keep 4 weekly backups  
    monthly_keep: int = 12  # Keep 12 monthly backups
    yearly_keep: int = 2    # Keep 2 yearly backups
```

---

## üõ†Ô∏è –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

### 1. **–û—Å–Ω–æ–≤–Ω–æ–π BackupService**

**–§–∞–π–ª:** `domain/services/backup_service.py`

```python
import asyncio
import gzip
import hashlib
import json
import shutil
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict
import tempfile
import os

class BackupService:
    def __init__(self):
        self.storage_backends = []
        self.scheduler = BackupScheduler(self)
        self.backup_dir = Path('backups')
        self.backup_dir.mkdir(exist_ok=True)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self._register_default_storages()
        
    def add_storage_backend(self, storage: BackupStorage):
        \"\"\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ backend –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è\"\"\"
        self.storage_backends.append(storage)
        
    async def create_full_backup(self) -> BackupInfo:
        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ backup\"\"\"
        backup_id = self._generate_backup_id('FULL')
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–ø–∫–∏ –¥–ª—è backup
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Backup –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
                await self._backup_database(temp_path / 'database.sql')
                
                # Backup –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                await self._backup_configuration(temp_path / 'config')
                
                # Backup –∫–ª—é—á–µ–π (–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω–æ)
                await self._backup_keys(temp_path / 'keys')
                
                # Backup –ª–æ–≥–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)
                await self._backup_logs(temp_path / 'logs')
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞
                archive_path = await self._create_archive(temp_path, backup_id)
                
                # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ checksum
                checksum = await self._calculate_checksum(archive_path)
                
                # Metadata
                metadata = BackupMetadata(
                    database_version='1.0',
                    config_version='2.1.0',
                    included_tables=['deals', 'orders', 'tickers'],
                    excluded_data=['temporary_data'],
                    compression_type='gzip',
                    encryption_enabled=False
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ storage backends
                storage_locations = []
                archive_size = os.path.getsize(archive_path)
                
                with open(archive_path, 'rb') as f:
                    archive_data = f.read()
                    
                for storage in self.storage_backends:
                    location = await storage.store_backup(archive_data, metadata)
                    storage_locations.append(location)
                    
                backup_info = BackupInfo(
                    backup_id=backup_id,
                    backup_type='FULL',
                    created_at=datetime.now(),
                    size_bytes=archive_size,
                    checksum=checksum,
                    metadata=metadata,
                    storage_location=';'.join(storage_locations)
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ info –æ backup
                await self._save_backup_info(backup_info)
                
                return backup_info
                
        except Exception as e:
            raise BackupException(f\"Full backup failed: {str(e)}\")
            
    async def create_incremental_backup(self, since: datetime) -> BackupInfo:
        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ backup\"\"\"
        backup_id = self._generate_backup_id('INCREMENTAL')
        
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # Backup —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–∏–≤—à–∏—Ö—Å—è –¥–∞–Ω–Ω—ã—Ö
                await self._backup_database_incremental(temp_path / 'database_changes.sql', since)
                
                # Backup –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥–æ–≤
                await self._backup_configuration_incremental(temp_path / 'config_changes', since)
                
                # Backup –Ω–æ–≤—ã—Ö –ª–æ–≥–æ–≤
                await self._backup_logs_incremental(temp_path / 'new_logs', since)
                
                # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞
                archive_path = await self._create_archive(temp_path, backup_id)
                checksum = await self._calculate_checksum(archive_path)
                
                metadata = BackupMetadata(
                    database_version='1.0',
                    config_version='2.1.0',
                    included_tables=['deals', 'orders', 'tickers'],
                    excluded_data=[],
                    compression_type='gzip',
                    encryption_enabled=False
                )
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                storage_locations = []
                archive_size = os.path.getsize(archive_path)
                
                with open(archive_path, 'rb') as f:
                    archive_data = f.read()
                    
                for storage in self.storage_backends:
                    location = await storage.store_backup(archive_data, metadata)
                    storage_locations.append(location)
                    
                backup_info = BackupInfo(
                    backup_id=backup_id,
                    backup_type='INCREMENTAL',
                    created_at=datetime.now(),
                    size_bytes=archive_size,
                    checksum=checksum,
                    metadata=metadata,
                    storage_location=';'.join(storage_locations)
                )
                
                await self._save_backup_info(backup_info)
                return backup_info
                
        except Exception as e:
            raise BackupException(f\"Incremental backup failed: {str(e)}\")
            
    async def restore_from_backup(self, backup_id: str) -> bool:
        \"\"\"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ backup\"\"\"
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ backup
            backup_info = await self._get_backup_info(backup_id)
            if not backup_info:
                raise BackupException(f\"Backup {backup_id} not found\")
                
            # –ó–∞–≥—Ä—É–∑–∫–∞ backup data
            backup_data = None
            for storage in self.storage_backends:
                try:
                    backup_data = await storage.retrieve_backup(backup_id)
                    if backup_data:
                        break
                except Exception:
                    continue
                    
            if not backup_data:
                raise BackupException(f\"Cannot retrieve backup data for {backup_id}\")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
            actual_checksum = hashlib.sha256(backup_data).hexdigest()
            if actual_checksum != backup_info.checksum:
                raise BackupException(f\"Backup checksum mismatch for {backup_id}\")
                
            # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                archive_path = temp_path / f\"{backup_id}.tar.gz\"
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞
                with open(archive_path, 'wb') as f:
                    f.write(backup_data)
                    
                # –†–∞—Å–ø–∞–∫–æ–≤–∫–∞
                extract_path = temp_path / 'extracted'
                await self._extract_archive(archive_path, extract_path)
                
                # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
                if backup_info.backup_type == 'FULL':
                    await self._restore_full_backup(extract_path)
                else:
                    await self._restore_incremental_backup(extract_path)
                    
            return True
            
        except Exception as e:
            print(f\"‚ùå Restore failed: {str(e)}\")
            return False
            
    async def _backup_database(self, output_path: Path):
        \"\"\"Backup –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö\"\"\"
        database_path = 'data/trading_bot.db'  # –ü—É—Ç—å –∫ –ë–î
        
        if os.path.exists(database_path):
            # SQLite backup
            with sqlite3.connect(database_path) as source:
                with sqlite3.connect(str(output_path)) as backup:
                    source.backup(backup)
        else:
            # –°–æ–∑–¥–∞—Ç—å –ø—É—Å—Ç–æ–π SQL —Ñ–∞–π–ª
            with open(output_path, 'w') as f:
                f.write('-- No database found\\n')
                
    async def _backup_configuration(self, output_dir: Path):
        \"\"\"Backup –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\"\"\"
        output_dir.mkdir(exist_ok=True)
        
        config_files = [
            'config/config.json',
            '.env',
            'security/salt.key'
        ]
        
        for config_file in config_files:
            if os.path.exists(config_file):
                dest_path = output_dir / Path(config_file).name
                shutil.copy2(config_file, dest_path)
                
    async def _backup_keys(self, output_dir: Path):
        \"\"\"Backup –∫–ª—é—á–µ–π –∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤\"\"\"
        output_dir.mkdir(exist_ok=True)
        
        keys_dir = Path('binance_keys')
        if keys_dir.exists():
            for key_file in keys_dir.glob('*'):
                if key_file.is_file():
                    shutil.copy2(key_file, output_dir / key_file.name)
                    
    async def _backup_logs(self, output_dir: Path):
        \"\"\"Backup –ª–æ–≥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏\"\"\"
        output_dir.mkdir(exist_ok=True)
        
        # Backup —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ª–æ–≥–∏
        cutoff_date = datetime.now() - timedelta(days=7)
        
        log_files = ['app.log', 'trading.log', 'security/audit.log']
        for log_file in log_files:
            if os.path.exists(log_file):
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –¥–∞—Ç–µ
                shutil.copy2(log_file, output_dir / Path(log_file).name)
                
    async def _create_archive(self, source_dir: Path, backup_id: str) -> Path:
        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Å–∂–∞—Ç–æ–≥–æ –∞—Ä—Ö–∏–≤–∞\"\"\"
        archive_path = self.backup_dir / f\"{backup_id}.tar.gz\"
        
        # –°–æ–∑–¥–∞–Ω–∏–µ tar.gz –∞—Ä—Ö–∏–≤–∞
        import tarfile
        with tarfile.open(archive_path, 'w:gz') as tar:
            for item in source_dir.rglob('*'):
                if item.is_file():
                    arcname = item.relative_to(source_dir)
                    tar.add(item, arcname=arcname)
                    
        return archive_path
        
    async def _calculate_checksum(self, file_path: Path) -> str:
        \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ checksum —Ñ–∞–π–ª–∞\"\"\"
        sha256_hash = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for byte_block in iter(lambda: f.read(4096), b\"\"):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
        
    def _generate_backup_id(self, backup_type: str) -> str:
        \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –¥–ª—è backup\"\"\"
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        return f\"{backup_type.lower()}_{timestamp}\"
        
    async def _save_backup_info(self, backup_info: BackupInfo):
        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ backup\"\"\"
        info_file = self.backup_dir / f\"{backup_info.backup_id}.json\"
        
        info_dict = {
            'backup_id': backup_info.backup_id,
            'backup_type': backup_info.backup_type,
            'created_at': backup_info.created_at.isoformat(),
            'size_bytes': backup_info.size_bytes,
            'checksum': backup_info.checksum,
            'storage_location': backup_info.storage_location,
            'metadata': {
                'database_version': backup_info.metadata.database_version,
                'config_version': backup_info.metadata.config_version,
                'included_tables': backup_info.metadata.included_tables,
                'compression_type': backup_info.metadata.compression_type
            }
        }
        
        with open(info_file, 'w') as f:
            json.dump(info_dict, f, indent=2)
            
    def _register_default_storages(self):
        \"\"\"–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è storage backends –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\"\"\"
        
        # Local file system storage
        self.add_storage_backend(LocalFileStorage(self.backup_dir / 'local'))
        
        # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å:
        # self.add_storage_backend(S3Storage(bucket='trading-bot-backups'))
        # self.add_storage_backend(GoogleCloudStorage(bucket='trading-bot-backups'))

# Storage backends
class LocalFileStorage(BackupStorage):
    def __init__(self, storage_dir: Path):
        self.storage_dir = storage_dir
        self.storage_dir.mkdir(exist_ok=True)
        
    async def store_backup(self, backup_data: bytes, metadata: BackupMetadata) -> str:
        backup_id = f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"
        backup_path = self.storage_dir / f\"{backup_id}.backup\"
        
        with open(backup_path, 'wb') as f:
            f.write(backup_data)
            
        return str(backup_path)
        
    async def retrieve_backup(self, backup_id: str) -> bytes:
        backup_path = self.storage_dir / f\"{backup_id}.backup\"
        
        if backup_path.exists():
            with open(backup_path, 'rb') as f:
                return f.read()
        else:
            raise BackupException(f\"Backup file not found: {backup_path}\")
            
    async def delete_backup(self, backup_id: str) -> bool:
        backup_path = self.storage_dir / f\"{backup_id}.backup\"
        
        if backup_path.exists():
            backup_path.unlink()
            return True
        return False

class BackupScheduler:
    def __init__(self, backup_service: BackupService):
        self.backup_service = backup_service
        self.schedules = []
        self.running = False
        self.scheduler_task = None
        
    async def start_scheduler(self):
        \"\"\"–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞\"\"\"
        if self.running:
            return
            
        self.running = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π
        await self._add_default_schedules()
        
    async def _add_default_schedules(self):
        \"\"\"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–π\"\"\"
        
        # –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π incremental backup –≤ 2:00
        daily_schedule = BackupSchedule(
            schedule_id='daily_incremental',
            backup_type='INCREMENTAL',
            cron_expression='0 2 * * *',  # Every day at 2 AM
            retention_days=7,
            enabled=True,
            storage_backends=['local']
        )
        self.schedules.append(daily_schedule)
        
        # –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π full backup –≤ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –≤ 3:00
        weekly_schedule = BackupSchedule(
            schedule_id='weekly_full',
            backup_type='FULL',
            cron_expression='0 3 * * 0',  # Every Sunday at 3 AM
            retention_days=28,
            enabled=True,
            storage_backends=['local']
        )
        self.schedules.append(weekly_schedule)
        
    async def _scheduler_loop(self):
        \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞\"\"\"
        while self.running:
            try:
                current_time = datetime.now()
                
                for schedule in self.schedules:
                    if schedule.enabled and self._should_run_backup(schedule, current_time):
                        await self._execute_scheduled_backup(schedule)
                        
            except Exception as e:
                print(f\"‚ùå Scheduler error: {str(e)}\")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 60 —Å–µ–∫—É–Ω–¥
            await asyncio.sleep(60)
            
    def _should_run_backup(self, schedule: BackupSchedule, current_time: datetime) -> bool:
        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å backup\"\"\"
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π cron parser
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —á–∞—Å
        
        if schedule.cron_expression == '0 2 * * *':  # Daily at 2 AM
            return current_time.hour == 2 and current_time.minute == 0
        elif schedule.cron_expression == '0 3 * * 0':  # Weekly Sunday at 3 AM
            return (current_time.weekday() == 6 and  # Sunday
                   current_time.hour == 3 and 
                   current_time.minute == 0)
        
        return False
        
    async def _execute_scheduled_backup(self, schedule: BackupSchedule):
        \"\"\"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ backup\"\"\"
        try:
            if schedule.backup_type == 'FULL':
                backup_info = await self.backup_service.create_full_backup()
            else:
                # Incremental since last backup
                last_backup_time = datetime.now() - timedelta(days=1)
                backup_info = await self.backup_service.create_incremental_backup(last_backup_time)
                
            print(f\"‚úÖ Scheduled {schedule.backup_type} backup completed: {backup_info.backup_id}\")
            
        except Exception as e:
            print(f\"‚ùå Scheduled backup failed: {str(e)}\")

class BackupException(Exception):
    \"\"\"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ backup –æ–ø–µ—Ä–∞—Ü–∏–π\"\"\"
    pass
```

---

## ‚úÖ –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏

### –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- [ ] Full –∏ incremental backup
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ backup
- [ ] –°–∂–∞—Ç–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
- [ ] –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ backup
- [ ] Multiple storage backends

### –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å:
- [ ] Backup –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ç–æ—Ä–≥–æ–≤–ª—é
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ backup
- [ ] Graceful handling –æ—à–∏–±–æ–∫
- [ ] Atomic backup –æ–ø–µ—Ä–∞—Ü–∏–∏

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
- [ ] Incremental backup < 30s
- [ ] Full backup < 5 –º–∏–Ω—É—Ç
- [ ] –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ < 10 –º–∏–Ω—É—Ç

---

## üöß –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

### –†–∏—Å–∫ 1: –ë–æ–ª—å—à–æ–π —Ä–∞–∑–º–µ—Ä backup —Ñ–∞–π–ª–æ–≤
**–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** –°–∂–∞—Ç–∏–µ, incremental backup, retention policy

### –†–∏—Å–∫ 2: Backup –≤–æ –≤—Ä–µ–º—è —Ç–æ—Ä–≥–æ–≤–ª–∏
**–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** Lock-free backup –º–µ—Ç–æ–¥—ã, off-peak scheduling

### –†–∏—Å–∫ 3: –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ backup
**–ú–∏—Ç–∏–≥–∞—Ü–∏—è:** Checksum validation, multiple storage locations

---

## üìö –°–≤—è–∑–∞–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

- Issue #6: DatabaseService
- [Database Backup Best Practices](https://www.postgresql.org/docs/current/backup.html)
- [3-2-1 Backup Strategy](https://www.veeam.com/blog/321-backup-rule.html)
